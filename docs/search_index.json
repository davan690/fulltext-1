[
["index.html", "fulltext manual Chapter 1 fulltext manual 1.1 Info 1.2 Citing fulltext 1.3 Installation", " fulltext manual built on 2019-12-13 - for fulltext v1.4.0 Chapter 1 fulltext manual An R package to search across and get full text for journal articles The fulltext package makes it easy to do text-mining by supporting the following steps: Search for articles Fetch articles Get links for full text articles (xml, pdf) Extract text from articles / convert formats Collect bits of articles that you actually need Download supplementary materials from papers 1.1 Info Code: https://github.com/ropensci/fulltext/ Issues/Bug reports: https://github.com/ropensci/fulltext/issues CRAN: https://cran.rstudio.com/web/packages/fulltext/ 1.2 Citing fulltext Scott Chamberlain (2019). fulltext: Full Text of ‘Scholarly’ Articles Across Many Data Sources. R package version 1.4.0. https://docs.ropensci.org/fulltext (website) https://github.com/ropensci/fulltext/ (devel) https://books.ropensci.org/fulltext/ (user manual) 1.3 Installation Stable version from CRAN install.packages(&quot;fulltext&quot;) Development version from GitHub devtools::install_github(&quot;ropensci/fulltext&quot;) Load library library(&#39;fulltext&#39;) "],
["intro.html", "Chapter 2 Introduction 2.1 User interface", " Chapter 2 Introduction 2.1 User interface Functions in fulltext are setup to make the package as easy to use as possible. The functions are organized around use cases: Search for articles Get full text links Get articles Get abstracts Pull out article sections of interest Because there are so many data sources for scholarly texts, it makes a lot of sense to simplify the details of each data source, and present a single user interface to all of them. "],
["data-sources.html", "Chapter 3 Data sources 3.1 Search 3.2 Abstracts 3.3 Links 3.4 Getting full text", " Chapter 3 Data sources Data sources in fulltext include: Crossref - via the rcrossref package Public Library of Science (PLOS) - via the rplos package Biomed Central arXiv - via the aRxiv package bioRxiv - via the biorxivr package PMC/Pubmed via Entrez - via the rentrez package Many more are supported via the above sources (e.g., Royal Society Open Science is available via Pubmed) We will add more, as publishers open up, and as we have time…See the master list here Data sources will differ by the task you are doing in fulltext. 3.1 Search When searching with ft_search() you’ll have access to a specific set of sources and no others, including: arxiv biorxivr bmc crossref entrez europe_pmc ma plos scopus You can see what plugins there are with ft_search_ls() 3.2 Abstracts When using ft_abstract() you have access to: crossref microsoft plos scopus You can see what plugins there are with ft_abstract_ls() 3.3 Links When using ft_links() to get links to full text, you’ll have access to: bmc cdc cogent copernicus crossref elife entrez frontiersin peerj plos rsoc You can see what plugins there are with ft_links_ls() 3.4 Getting full text While using ft_get() to fetch full text of articles you’ll have access to a set of specific data sources (in this case publishers) for which we have some coded plugins (i.e., functions): aaas aip amersocclinoncol amersocmicrobiol arxiv biorxiv bmc cambridge copernicus crossref elife elsevier entrez frontiersin ieee informa instinvestfil jama microbiology peerj pensoft plos pnas royalsocchem sciencedirect scientificsocieties wiley You can see what plugins there are with ft_get_ls() But there are also other options within ft_get() that we take advantage of. This is because DOIs (Digital Object Identifiers) which you feed into ft_get() have a prefix that is affiliated with a specific publisher. We can then decide whether to use one of our plugins listed in ft_get_ls() or something else. If we don’t have a plugin we first look to see if Crossref has the full text link to either XML or PDF for the DOI. If not, we then go to an API rOpenSci maintains at https://ftdoi.org1. This API has a set of rules for each publisher - some of which are simple rules like add a URL plus a DOI - but some require an HTTP request then some string manipulation. You can use the ftdoi API from R with the https://github.com/ropenscilabs/rftdoi package.↩︎ "],
["authentication.html", "Chapter 4 Authentication", " Chapter 4 Authentication Some data sources require authentication. Here’s a breakdown of how to do authentication by data source: BMC: BMC is integrated into Springer Publishers now, and that API requires an API key. Get your key by signing up at https://dev.springer.com/, then you’ll get a key. Pass the key to a named parameter key to bmcopts. Or, save your key in your .Renviron file as SPRINGER_KEY, and we’ll read it in for you, and you don’t have to pass in anything. Scopus: Scopus requires an API key to search their service. Go to https://dev.elsevier.com/index.html, register for an account, then when you’re in your account, create an API key. Pass in as variable key to scopusopts, or store your key under the name ELSEVIER_SCOPUS_KEY as an environment variable in .Renviron, and we’ll read it in for you. See ?Startup in R for help. Microsoft: Get a key by creating an Azure account at https://www.microsoft.com/cognitive-services/en-us/subscriptions, then requesting a key for Academic Knowledge API within Cognitive Services. Store it as an environment variable in your .Renviron file - see [Startup] for help. Pass your API key into maopts as a named element in a list like list(key = Sys.getenv('MICROSOFT_ACADEMIC_KEY')) Crossref: Crossref encourages requests with contact information (an email address) and will forward you to a dedicated API cluster for improved performance when you share your email address with them. https://github.com/CrossRef/rest-api-doc#good-manners--more-reliable-service To pass your email address to Crossref via this client, store it as an environment variable in .Renviron like crossref_email = name@example.com Entrez: NCBI limits users to making only 3 requests per second. But, users who register for an API key are able to make up to ten requests per second. Getting a key is simple; register for a “my ncbi” account then click on a button in the account settings page. Once you have an API key, you can pass it as the argument api_key to entrezopts in both [ft_get()] and [ft_search()]. However, we advise you use environment variables instead as they are more secure. To do that you can set an environment variable for the current R session like Sys.setenv(ENTREZ_KEY=\"yourkey\") OR better yet set it in your .Renviron or equivalent file with an entry like ENTREZ_KEY=yourkey so that it is used across R sessions. Not needed for PLOS, eLife, arxiv, biorxiv, Euro PMC "],
["search.html", "Chapter 5 Search 5.1 Usage", " Chapter 5 Search Search is what you’ll likely start with for a number of reasons. First, search functionality in fulltext means that you can start from searching on words like ‘ecology’ or ‘cellular’ - and the output of that search can be fed downstream to the next major task: fetching articles. 5.1 Usage library(fulltext) List backends available ft_search_ls() #&gt; [1] &quot;arxiv&quot; &quot;biorxivr&quot; &quot;bmc&quot; &quot;crossref&quot; &quot;entrez&quot; #&gt; [6] &quot;europe_pmc&quot; &quot;ma&quot; &quot;plos&quot; &quot;scopus&quot; Search - by default searches against PLOS (Public Library of Science) res &lt;- ft_search(query = &quot;ecology&quot;) The output of ft_search is a ft S3 object, with a summary of the results: res #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 50330; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] and has slots for each data source: names(res) #&gt; [1] &quot;plos&quot; &quot;bmc&quot; &quot;crossref&quot; &quot;entrez&quot; &quot;arxiv&quot; &quot;biorxiv&quot; &quot;europmc&quot; #&gt; [8] &quot;scopus&quot; &quot;ma&quot; Get data for a single source res$plos #&gt; Query: [ecology] #&gt; Records found, returned: [50330, 10] #&gt; License: [CC-BY] #&gt; id #&gt; 1 10.1371/journal.pone.0001248 #&gt; 2 10.1371/journal.pone.0059813 #&gt; 3 10.1371/journal.pone.0080763 #&gt; 4 10.1371/journal.pone.0220747 #&gt; 5 10.1371/journal.pone.0155019 #&gt; 6 10.1371/journal.pone.0175014 #&gt; 7 10.1371/journal.pone.0150648 #&gt; 8 10.1371/journal.pone.0208370 #&gt; 9 10.1371/journal.pcbi.1003594 #&gt; 10 10.1371/journal.pone.0102437 "],
["abstract.html", "Chapter 6 Abstract 6.1 Usage", " Chapter 6 Abstract Fetching abstracts likely will come after searching for articles with ft_search(). There are a few scenarios in which simply getting abstracts in lieu of full text may be enough. For example, if you know that a large portion of the articles you want to mine text from are closed access and you don’t have access to them, you may have access to the abstracts depending on the publisher. In addition, there are cases in which you really only need abstracts regardless of whether full text is available or not. ft_abstract() gives you access to the following data sources: crossref microsoft plos scopus 6.1 Usage library(fulltext) List data sources available ft_abstract_ls() #&gt; [1] &quot;crossref&quot; &quot;microsoft&quot; &quot;plos&quot; &quot;scopus&quot; Search - by default searches against PLOS (Public Library of Science) res &lt;- ft_search(query = &quot;ecology&quot;) (dois &lt;- res$plos$data$id) #&gt; [1] &quot;10.1371/journal.pone.0001248&quot; &quot;10.1371/journal.pone.0059813&quot; #&gt; [3] &quot;10.1371/journal.pone.0080763&quot; &quot;10.1371/journal.pone.0220747&quot; #&gt; [5] &quot;10.1371/journal.pone.0155019&quot; &quot;10.1371/journal.pone.0175014&quot; #&gt; [7] &quot;10.1371/journal.pone.0150648&quot; &quot;10.1371/journal.pone.0208370&quot; #&gt; [9] &quot;10.1371/journal.pcbi.1003594&quot; &quot;10.1371/journal.pone.0102437&quot; Take the output of ft_search() and pass to ft_abstract(): out &lt;- ft_abstract(dois) out #&gt; &lt;fulltext abstracts&gt; #&gt; Found: #&gt; [PLOS: 10; Scopus: 0; Microsoft: 0; Crossref: 0] The output has slots for each data source: names(out) #&gt; [1] &quot;plos&quot; &quot;scopus&quot; &quot;ma&quot; &quot;crossref&quot; Index to the data source you want to get data from, here selecting the first item: out$plos[[1]] #&gt; $doi #&gt; [1] &quot;10.1371/journal.pone.0001248&quot; #&gt; #&gt; $abstract #&gt; [1] &quot;Background: Soil ecology has produced a huge corpus of results on relations between soil organisms, ecosystem processes controlled by these organisms and links between belowground and aboveground processes. However, some soil scientists think that soil ecology is short of modelling and evolutionary approaches and has developed too independently from general ecology. We have tested quantitatively these hypotheses through a bibliographic study (about 23000 articles) comparing soil ecology journals, generalist ecology journals, evolutionary ecology journals and theoretical ecology journals. Findings: We have shown that soil ecology is not well represented in generalist ecology journals and that soil ecologists poorly use modelling and evolutionary approaches. Moreover, the articles published by a typical soil ecology journal (Soil Biology and Biochemistry) are cited by and cite low percentages of articles published in generalist ecology journals, evolutionary ecology journals and theoretical ecology journals. Conclusion: This confirms our hypotheses and suggests that soil ecology would benefit from an effort towards modelling and evolutionary approaches. This effort should promote the building of a general conceptual framework for soil ecology and bridges between soil ecology and general ecology. We give some historical reasons for the parsimonious use of modelling and evolutionary approaches by soil ecologists. We finally suggest that a publication system that classifies journals according to their Impact Factors and their level of generality is probably inadequate to integrate “particularity” (empirical observations) and “generality” (general theories), which is the goal of all natural sciences. Such a system might also be particularly detrimental to the development of a science such as ecology that is intrinsically multidisciplinary. &quot; Which gives a named list, with the DOI as the first element, then the abstract as a single character string. You can then take these abstracts and use any number of R packages for text mining. "],
["links.html", "Chapter 7 Links 7.1 Usage", " Chapter 7 Links The ft_links function makes it easy to get URLs for full text versions of articles. You can for instance only use fulltext to pass DOIs directly to ft_links to get URLs to use elsewhere in your research workflow. Or you may want to search first with ft_search, then pass that output directly to ft_links. 7.1 Usage library(fulltext) List backends available ft_links_ls() #&gt; [1] &quot;bmc&quot; &quot;cdc&quot; &quot;cogent&quot; &quot;copernicus&quot; &quot;crossref&quot; #&gt; [6] &quot;elife&quot; &quot;entrez&quot; &quot;frontiersin&quot; &quot;peerj&quot; &quot;plos&quot; #&gt; [11] &quot;rsoc&quot; You can pass DOIs directly to ft_links (res &lt;- ft_links(&#39;10.3389/fphar.2014.00109&#39;)) #&gt; &lt;fulltext links&gt; #&gt; [Found] 1 #&gt; [IDs] 10.3389/fphar.2014.00109 ... res$frontiersin #&gt; $found #&gt; [1] 1 #&gt; #&gt; $ids #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; #&gt; #&gt; $data #&gt; $data$`10.3389/fphar.2014.00109` #&gt; $data$`10.3389/fphar.2014.00109`$xml #&gt; [1] &quot;http://journal.frontiersin.org/article/10.3389/fphar.2014.00109/xml/nlm&quot; #&gt; #&gt; $data$`10.3389/fphar.2014.00109`$pdf #&gt; [1] &quot;http://journal.frontiersin.org/article/10.3389/fphar.2014.00109/pdf&quot; #&gt; #&gt; #&gt; #&gt; $opts #&gt; list() Or search first (res1 &lt;- ft_search(query=&#39;ecology&#39;, from=&#39;entrez&#39;)) #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 185437; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 10; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] Then pass the output of that directly to ft_links (out &lt;- ft_links(res1)) #&gt; &lt;fulltext links&gt; #&gt; [Found] 6 #&gt; [IDs] ID_31822672 ID_31819142 ID_31819128 ID_31819121 ID_31819107 ID_31818368 #&gt; ... "],
["fetch.html", "Chapter 8 Fetch 8.1 Data formats 8.2 How data is stored 8.3 Usage 8.4 Errors 8.5 Cleanup 8.6 Internals 8.7 Notes about specific data sources", " Chapter 8 Fetch The ft_get function makes it easy to fetch full text articles. There are a few different ways to use ft_get: Pass in only DOIs - leave from parameter NULL. This route will first query Crossref API for the publisher of the DOI, then we’ll use the appropriate method to fetch full text from the publisher. If a publisher is not found for the DOI, then we’ll throw back a message telling you a publisher was not found. Pass in DOIs (or other pub IDs) and use the from parameter. This route means we don’t have to make an extra API call to Crossref (thus, this route is faster) to determine the publisher for each DOI. We go straight to getting full text based on the publisher. Use ft_search() to search for articles. Then pass that output to this function, which will use info in that object. This behaves the same as the previous option in that each DOI has publisher info so we know how to get full text for each DOI. Note that some publishers are available through other data sources, e.g., through Entrez’s Pubmed. ft_get is a bit complicated. These are just some of the hurdles we’re jumping over: Negotiating various user inputs, likely seeing new publishers we’ve not dealt with Dealing with authentication and trying to make it easier for users Users sometimes being at an IP address that has access to a publisher and sometimes not Caching results to avoid unnecessary downloads if the content has already been acquired Thus, expect some hiccups here, and please do report problems, and if a certain publisher is not supported yet. 8.1 Data formats You can specify whether you want PDF, XML or plaint text with the type parameter. It is sometimes ignored, sometimes used, depending on the data source. For certain data sources, they only accept one type. Details by data source/publisher: PLOS: pdf and xml Entrez: only xml eLife: pdf and xml Pensoft: pdf and xml arXiv: only pdf BiorXiv: only pdf Elsevier: pdf and plain Wiley: only pdf Peerj: pdf and xml Informa: only pdf FrontiersIn: pdf and xml Copernicus: pdf and xml Scientific Societies: only pdf Crossref: depends on the publisher other data sources/publishers: there are too many to cover here - will try to make a helper in the future for what is covered by different publishers 8.2 How data is stored This depends on what backend value you use. If you use the default (rds) we store all data in .rds files. These are binary compressed files that are specific to R. Because they are specific to R, you don’t want to use this option if part of your downstream workflow is using another tool/programming language. The three types are stored in differnt ways. xml and plain text are parsed to plain text then stored with whatever backend you choose. However, pdf is retrived as raw bytes and stored as such. Thus, we no longer write pdf files to disk. However, you can easily do that yourself with ft_extract() or yourself by using pdftools::pdf_text which accepts a file path to a pdf or raw bytes. 8.3 Usage library(fulltext) List backends available ft_get_ls() #&gt; [1] &quot;aaas&quot; &quot;aip&quot; &quot;amersocclinoncol&quot; #&gt; [4] &quot;amersocmicrobiol&quot; &quot;arxiv&quot; &quot;biorxiv&quot; #&gt; [7] &quot;bmc&quot; &quot;cambridge&quot; &quot;copernicus&quot; #&gt; [10] &quot;crossref&quot; &quot;elife&quot; &quot;elsevier&quot; #&gt; [13] &quot;entrez&quot; &quot;frontiersin&quot; &quot;ieee&quot; #&gt; [16] &quot;informa&quot; &quot;instinvestfil&quot; &quot;jama&quot; #&gt; [19] &quot;microbiology&quot; &quot;peerj&quot; &quot;pensoft&quot; #&gt; [22] &quot;plos&quot; &quot;pnas&quot; &quot;royalsocchem&quot; #&gt; [25] &quot;sciencedirect&quot; &quot;scientificsocieties&quot; &quot;wiley&quot; The simplest approach is passing a DOI directly to ft_get (res &lt;- ft_get(&#39;10.1371/journal.pone.0086169&#39;)) #&gt; &lt;fulltext text&gt; #&gt; [Docs] 1 #&gt; [Source] ext - /Users/sckott/Library/Caches/R/fulltext #&gt; [IDs] 10.1371/journal.pone.0086169 ... res$plos #&gt; $found #&gt; [1] 1 #&gt; #&gt; $dois #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $data #&gt; $data$backend #&gt; [1] &quot;ext&quot; #&gt; #&gt; $data$cache_path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext&quot; #&gt; #&gt; $data$path #&gt; $data$path$`10.1371/journal.pone.0086169` #&gt; $data$path$`10.1371/journal.pone.0086169`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_1371_journal_pone_0086169.xml&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$id #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$error #&gt; NULL #&gt; #&gt; #&gt; #&gt; $data$data #&gt; NULL #&gt; #&gt; #&gt; $opts #&gt; $opts$doi #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $opts$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $opts$progress #&gt; [1] FALSE #&gt; #&gt; #&gt; $errors #&gt; id error #&gt; 1 10.1371/journal.pone.0086169 &lt;NA&gt; You can pass many DOIs in at once (res &lt;- ft_get(c(&#39;10.3389/fphar.2014.00109&#39;, &#39;10.3389/feart.2015.00009&#39;))) #&gt; &lt;fulltext text&gt; #&gt; [Docs] 2 #&gt; [Source] ext - /Users/sckott/Library/Caches/R/fulltext #&gt; [IDs] 10.3389/fphar.2014.00109 10.3389/feart.2015.00009 ... res$frontiersin #&gt; $found #&gt; [1] 2 #&gt; #&gt; $dois #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $data #&gt; $data$backend #&gt; [1] &quot;ext&quot; #&gt; #&gt; $data$cache_path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext&quot; #&gt; #&gt; $data$path #&gt; $data$path$`10.3389/fphar.2014.00109` #&gt; $data$path$`10.3389/fphar.2014.00109`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_3389_fphar_2014_00109.xml&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$id #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$error #&gt; NULL #&gt; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009` #&gt; $data$path$`10.3389/feart.2015.00009`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_3389_feart_2015_00009.xml&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$id #&gt; [1] &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$error #&gt; NULL #&gt; #&gt; #&gt; #&gt; $data$data #&gt; NULL #&gt; #&gt; #&gt; $opts #&gt; $opts$dois #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $opts$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $opts$progress #&gt; [1] FALSE #&gt; #&gt; #&gt; $errors #&gt; id error #&gt; 1 10.3389/fphar.2014.00109 &lt;NA&gt; #&gt; 2 10.3389/feart.2015.00009 &lt;NA&gt; 8.4 Errors ft_get() for each article has an error slot. If the error slot is NULL then there is no error. If the error slot is not NULL there was an error, and the error message will be a character string in that slot. Possible errors include: An error reported by the web service e.g. “Timeout was reached: Connection timed out after 10003 milliseconds” No link found e.g. “no link found from Crossref” OR “has no link available” We attempted to fetch the article but the content type wasn’t what was expected. In this case we skip to the next article. e.g. “type was supposed to be pdf, but was text/html; charset=UTF-8” Weird uninformative errors e.g. “Recv failure: Operation timed out” OR “Operation was aborted by an application callback” An error associated mostly with PLOS. PLOS gives DOIs for parts of articles, like figures, so it doesn’t make sense to get full text of a figure. e.g., “was not found or may be a DOI for a part of an article” An additional top level slot called errors has a data.frame of all errors from each article, like: res &lt;- ft_get(c(&#39;10.7554/eLife.03032&#39;, &#39;10.7554/eLife.aaaa&#39;), from = &quot;elife&quot;) res$elife$errors id error 1 10.7554/eLife.03032 &lt;NA&gt; 2 10.7554/eLife.aaaa subscript out of bounds Where the second DOI was invalid. Granted the error in the data.frame “subscript out of bounds” isn’t very informative, but we can work on that. 8.5 Cleanup The above section about errors suggests that we often run into errors. When we run into errors downloading full text we capture the error message, if there is one, and delete the file we were trying to create. That is, we cleanup upon hitting an error such that you shouldn’t end up with blank files on your machine. Let us know if this isn’t true in your case and we’ll get it fixed. Note that even if you exit out of the all to ft_get() it should clean up a file if it is not completey done creating it, so you shouldn’t end up with bad files if you exit out of the function when it’s running. 8.6 Internals What’s going on under the hood in ft_get()? It goes like this: If you request data from a specific data source (use of from, only allowed for PLOS, Entrez, eLife, Pensoft, arXiv, BiorXiv, Elsevier and Wiley): Grab publisher specific collector function Each of these functions has specific code for that publisher to pull full text given an article identifier If you don’t request a specific data source: Guess which publisher the DOI comes from If publisher discovered that we have plugins for Get publisher specific collector function If publisher not discovered Ping the https://ftdoi.org API If publisher found with ftdoi API: Link for full text used from the ftdoi API response If publisher not found with ftdoi API: Attempt fetch via Crossref API - if links found we try those, some work and some don’t 8.7 Notes about specific data sources 8.7.1 Elsevier When you don’t have access to the full text of Elsevier articles they will often still give you something, but it will sometimes be just metadata of the paper or sometimes an abstract if you’re lucky. When you go to extract the text this will be rather obvious. "],
["chunks.html", "Chapter 9 Extracting text 9.1 Usage 9.2 Tabularize 9.3 Other inputs", " Chapter 9 Extracting text Functions for extracting parts of texts used to live inside of fulltext, but have now moved to the package pubchunks. The pubchunks::pub_chunks function tries to make it easy to extract the parts of articles you want. This only works with XML format articles though since although we can get text out of PDFs, there is no machine readable way to say “I want the abstract”. In addition to only working with XML, this function only has knowledge about a select set of publishers for which we’ve encoded knowledge about how to get different sections of the article. Not all publishers use the same format XML - so each publisher is slightly different for how to get to each section. That is, to get to the abstract requires slightly different xpath for publisher A vs. publisher B vs. publisher C. An alternative to pubchunks is to use xpath or css selectors yourself to slice and dice XML. 9.1 Usage library(fulltext) library(pubchunks) Get a full text article x &lt;- ft_get(&#39;10.1371/journal.pone.0086169&#39;, from=&#39;plos&#39;) Note that unlike previous versions of fulltext you now have to collect (ft_collect()) the text from the XML file on disk. Then you can pass to pub_chunks(), here to get authors. x %&gt;% ft_collect %&gt;% pub_chunks(&quot;authors&quot;) #&gt; $plos #&gt; $plos$`10.1371/journal.pone.0086169` #&gt; &lt;pub chunks&gt; #&gt; from: xml_document #&gt; publisher/journal: plos/PLoS ONE #&gt; sections: authors #&gt; showing up to first 5: #&gt; authors (n=4): nested list #&gt; #&gt; #&gt; attr(,&quot;ft_data&quot;) #&gt; [1] TRUE In another example, let’s search for PLOS articles. library(&quot;rplos&quot;) (dois &lt;- searchplos(q=&quot;*:*&quot;, fl=&#39;id&#39;, fq=list(&#39;doc_type:full&#39;,&quot;article_type:\\&quot;research article\\&quot;&quot;), limit=5)$data$id) #&gt; [1] &quot;10.1371/journal.pone.0020843&quot; &quot;10.1371/journal.pone.0022257&quot; #&gt; [3] &quot;10.1371/journal.pone.0023139&quot; &quot;10.1371/journal.pone.0023138&quot; #&gt; [5] &quot;10.1371/journal.pone.0023119&quot; Then get the full text x &lt;- ft_get(dois, from=&quot;plos&quot;) Then pull out various sections of each article. remember to pull out the full text first x &lt;- ft_collect(x) x %&gt;% pub_chunks(&quot;front&quot;) x %&gt;% pub_chunks(&quot;body&quot;) x %&gt;% pub_chunks(&quot;back&quot;) x %&gt;% pub_chunks(&quot;history&quot;) x %&gt;% pub_chunks(&quot;authors&quot;) x %&gt;% pub_chunks(c(&quot;doi&quot;,&quot;categories&quot;)) x %&gt;% pub_chunks(&quot;all&quot;) x %&gt;% pub_chunks(&quot;publisher&quot;) x %&gt;% pub_chunks(&quot;acknowledgments&quot;) x %&gt;% pub_chunks(&quot;permissions&quot;) x %&gt;% pub_chunks(&quot;journal_meta&quot;) x %&gt;% pub_chunks(&quot;article_meta&quot;) 9.2 Tabularize The function pub_tabularize() is useful for coercing the output of pub_chunks() into a data.frame, the lingua franca of data work in R. library(data.table) x &lt;- pub_chunks(x, c(&quot;doi&quot;, &quot;title&quot;)) x &lt;- pub_tabularize(x) rbindlist(x$plos, fill = TRUE) #&gt; doi #&gt; 1: 10.1371/journal.pone.0020843 #&gt; 2: 10.1371/journal.pone.0022257 #&gt; 3: 10.1371/journal.pone.0023139 #&gt; 4: 10.1371/journal.pone.0023138 #&gt; 5: 10.1371/journal.pone.0023119 #&gt; title #&gt; 1: New Insights into the Evolution of Wolbachia Infections in Filarial Nematodes Inferred from a Large Range of Screened Species #&gt; 2: Dynamic Distribution of Nuclear Coactivator 4 during Mitosis: Association with Mitotic Apparatus and Midbodies #&gt; 3: Contrasting Expression of Canonical Wnt Signaling Reporters TOPGAL, BATGAL and Axin2LacZ during Murine Lung Development and Repair #&gt; 4: Regrets Associated with Providing Healthcare: Qualitative Study of Experiences of Hospital-Based Physicians and Nurses #&gt; 5: Strategies for Enhancing the Accumulation and Retention of Extracellular Matrix in Tissue-Engineered Cartilage Cultured in Bioreactors #&gt; .publisher #&gt; 1: plos #&gt; 2: plos #&gt; 3: plos #&gt; 4: plos #&gt; 5: plos 9.3 Other inputs pub_chunks() works with other inputs besides the output of fulltext::ft_get(). 9.3.1 Files x &lt;- system.file(&quot;examples/10_1016_0021_8928_59_90156_x.xml&quot;, package = &quot;pubchunks&quot;) pub_chunks(x, &quot;abstract&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: file #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: abstract #&gt; showing up to first 5: #&gt; abstract (n=1): Abstract #&gt; #&gt; This pa ... pub_chunks(x, &quot;title&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: file #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: title #&gt; showing up to first 5: #&gt; title (n=1): On the driving of a piston with a rigid collar int ... pub_chunks(x, &quot;authors&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: file #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: authors #&gt; showing up to first 5: #&gt; authors (n=1): Chetaev, D.N pub_chunks(x, c(&quot;title&quot;, &quot;refs&quot;)) #&gt; &lt;pub chunks&gt; #&gt; from: file #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: title, refs #&gt; showing up to first 5: #&gt; title (n=1): On the driving of a piston with a rigid collar int ... #&gt; refs (n=6): 1.G.N.WatsonTeoriia besselevykh funktsiiTheory of The output of pub_chunks() is a list with an S3 class pub_chunks to make internal work in the package easier. You can easily see the list structure by using unclass(). 9.3.2 xml in a string xml &lt;- paste0(readLines(x), collapse = &quot;&quot;) pub_chunks(xml, &quot;title&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: character #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: title #&gt; showing up to first 5: #&gt; title (n=1): On the driving of a piston with a rigid collar int ... 9.3.3 xml2 objects xml &lt;- paste0(readLines(x), collapse = &quot;&quot;) xml &lt;- xml2::read_xml(xml) pub_chunks(xml, &quot;title&quot;) #&gt; &lt;pub chunks&gt; #&gt; from: xml_document #&gt; publisher/journal: elsevier/Journal of Applied Mathematics and Mechanics #&gt; sections: title #&gt; showing up to first 5: #&gt; title (n=1): On the driving of a piston with a rigid collar int ... "],
["table.html", "Chapter 10 Table 10.1 Usage", " Chapter 10 Table The ft_table() function makes it easy to create a data.frame of the text of PDF, plain text, and XML files, together with DOIs/IDs for each article. It’s similar to the readtext::readtext() function, but is much more specific to just this package. With the output of ft_table() you can go directly into a text-mining package like quanteda. 10.1 Usage library(fulltext) Use ft_table() to pull out text from all articles. ft_table() #&gt; # A tibble: 33 x 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1002/ecy… 10_1002_ecy_1… &quot; … /Users/sckott/Library/… #&gt; 2 10.1017/s09… 10_1017_s0922… &quot;Downloaded from https:/… /Users/sckott/Library/… #&gt; 3 10.1111/204… 10_1111_2041_… &quot;&quot; /Users/sckott/Library/… #&gt; 4 10.1371/jou… 10_1371_journ… plosPLoS Comput Biolplos… /Users/sckott/Library/… #&gt; 5 10.1371/jou… 10_1371_journ… plosPLoS Comput Biolplos… /Users/sckott/Library/… #&gt; 6 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 7 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 8 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 9 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 10 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; # … with 23 more rows You can pull out just text from XML files ft_table(type = &quot;xml&quot;) #&gt; # A tibble: 28 x 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1111/204… 10_1111_2041_… &quot;&quot; /Users/sckott/Library/… #&gt; 2 10.1371/jou… 10_1371_journ… plosPLoS Comput Biolplos… /Users/sckott/Library/… #&gt; 3 10.1371/jou… 10_1371_journ… plosPLoS Comput Biolplos… /Users/sckott/Library/… #&gt; 4 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 5 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 6 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 7 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 8 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 9 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; 10 10.1371/jou… 10_1371_journ… PLoS ONEplosplosonePLoS … /Users/sckott/Library/… #&gt; # … with 18 more rows You can pull out just text from PDF files ft_table(type = &quot;pdf&quot;) #&gt; # A tibble: 5 x 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1002/ec… 10_1002_ecy_1… &quot; … /Users/sckott/Library/C… #&gt; 2 10.1017/s0… 10_1017_s0922… &quot;Downloaded from https://… /Users/sckott/Library/C… #&gt; 3 10.2307/19… 10_2307_19299… &quot;July, 1957 … /Users/sckott/Library/C… #&gt; 4 10.2307/19… 10_2307_19431… &quot;&quot; /Users/sckott/Library/C… #&gt; 5 cond-mat/9… cond_mat_9309… &quot; … /Users/sckott/Library/C… You can pull out XML but not extract the text. So you’ll get XML strings that you can parse yourself with xpath/css selectors/etc. ft_table(xml_extract_text = FALSE) #&gt; # A tibble: 33 x 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1002/ecy… 10_1002_ecy_1… &quot; … /Users/sckott/Library/… #&gt; 2 10.1017/s09… 10_1017_s0922… &quot;Downloaded from https:/… /Users/sckott/Library/… #&gt; 3 10.1111/204… 10_1111_2041_… &quot;%PDF-1.6\\n%\\xbf\\xf7\\xa2… /Users/sckott/Library/… #&gt; 4 10.1371/jou… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\&quot; e… /Users/sckott/Library/… #&gt; 5 10.1371/jou… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\&quot; e… /Users/sckott/Library/… #&gt; 6 10.1371/jou… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\&quot; e… /Users/sckott/Library/… #&gt; 7 10.1371/jou… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\&quot; e… /Users/sckott/Library/… #&gt; 8 10.1371/jou… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\&quot; e… /Users/sckott/Library/… #&gt; 9 10.1371/jou… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\&quot; e… /Users/sckott/Library/… #&gt; 10 10.1371/jou… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\&quot; e… /Users/sckott/Library/… #&gt; # … with 23 more rows "],
["supplementary.html", "Chapter 11 Supplementary", " Chapter 11 Supplementary coming soon … "],
["use-cases.html", "Chapter 12 Use cases", " Chapter 12 Use cases Coming soon … "],
["literature.html", "Chapter 13 Literature", " Chapter 13 Literature Here is a review of existing methods. "]
]
