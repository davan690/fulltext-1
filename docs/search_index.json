[
["index.html", "fulltext manual Chapter 1 fulltext manual 1.1 Info 1.2 Citing fulltext 1.3 Installation", " fulltext manual built on 2019-04-06 - for fulltext v1.2.0 Chapter 1 fulltext manual An R package to search across and get full text for journal articles The fulltext package makes it easy to do text-mining by supporting the following steps: Search for articles Fetch articles Get links for full text articles (xml, pdf) Extract text from articles / convert formats Collect bits of articles that you actually need Download supplementary materials from papers 1.1 Info Code: https://github.com/ropensci/fulltext/ Issues/Bug reports: https://github.com/ropensci/fulltext/issues CRAN: https://cran.rstudio.com/web/packages/fulltext/ 1.2 Citing fulltext Scott Chamberlain &amp; Will Pearse (2018). fulltext: Full Text of ‘Scholarly’ Articles Across Many Data Sources. R package version 1.1.0. https://github.com/ropensci/fulltext 1.3 Installation Stable version from CRAN install.packages(&quot;fulltext&quot;) Development version from GitHub devtools::install_github(&quot;ropensci/fulltext&quot;) Load library library(&#39;fulltext&#39;) "],
["intro.html", "Chapter 2 Introduction 2.1 User interface", " Chapter 2 Introduction 2.1 User interface Functions in fulltext are setup to make the package as easy to use as possible. The functions are organized around use cases: Search for articles Get full text links Get articles Get abstracts Pull out article sections of interest Because there are so many data sources for scholarly texts, it makes a lot of sense to simplify the details of each data source, and present a single user interface to all of them. "],
["data-sources.html", "Chapter 3 Data sources 3.1 Search 3.2 Abstracts 3.3 Links 3.4 Getting full text", " Chapter 3 Data sources Data sources in fulltext include: Crossref - via the rcrossref package Public Library of Science (PLOS) - via the rplos package Biomed Central arXiv - via the aRxiv package bioRxiv - via the biorxivr package PMC/Pubmed via Entrez - via the rentrez package Many more are supported via the above sources (e.g., Royal Society Open Science is available via Pubmed) We will add more, as publishers open up, and as we have time…See the master list here Data sources will differ by the task you are doing in fulltext. 3.1 Search When searching with ft_search() you’ll have access to a specific set of sources and no others, including: arxiv biorxivr bmc crossref entrez europe_pmc ma plos scopus You can see what plugins there are with ft_search_ls() 3.2 Abstracts When using ft_abstract() you have access to: crossref microsoft plos scopus You can see what plugins there are with ft_abstract_ls() 3.3 Links When using ft_links() to get links to full text, you’ll have access to: bmc cogent copernicus crossref elife entrez frontiersin peerj plos You can see what plugins there are with ft_links_ls() 3.4 Getting full text While using ft_get() to fetch full text of articles you’ll have access to a set of specific data sources (in this case publishers) for which we have some coded plugins (i.e., functions): aaas aip amersocclinoncol amersocmicrobiol arxiv biorxiv bmc copernicus crossref elife elsevier entrez frontiersin ieee informa instinvestfil jama microbiology peerj pensoft plos pnas royalsocchem scientificsocieties wiley You can see what plugins there are with ft_get_ls() But there are also other options within ft_get() that we take advantage of. This is because DOIs (Digital Object Identifiers) which you feed into ft_get() have a prefix that is affiliated with a specific publisher. We can then decide whether to use one of our plugins listed in ft_get_ls() or something else. If we don’t have a plugin we first look to see if Crossref has the full text link to either XML or PDF for the DOI. If not, we then go to an API rOpenSci maintains at https://ftdoi.org1. This API has a set of rules for each publisher - some of which are simple rules like add a URL plus a DOI - but some require an HTTP request then some string manipulation. You can use the ftdoi API from R with the https://github.com/ropenscilabs/rftdoi package.↩ "],
["authentication.html", "Chapter 4 Authentication", " Chapter 4 Authentication Some data sources require authentication. Here’s a breakdown of how to do authentication by data source: BMC: BMC is integrated into Springer Publishers now, and that API requires an API key. Get your key by signing up at https://dev.springer.com/, then you’ll get a key. Pass the key to a named parameter key to bmcopts. Or, save your key in your .Renviron file as SPRINGER_KEY, and we’ll read it in for you, and you don’t have to pass in anything. Scopus: Scopus requires an API key to search their service. Go to https://dev.elsevier.com/index.html, register for an account, then when you’re in your account, create an API key. Pass in as variable key to scopusopts, or store your key under the name ELSEVIER_SCOPUS_KEY as an environment variable in .Renviron, and we’ll read it in for you. See ?Startup in R for help. Microsoft: Get a key by creating an Azure account at https://www.microsoft.com/cognitive-services/en-us/subscriptions, then requesting a key for Academic Knowledge API within Cognitive Services. Store it as an environment variable in your .Renviron file - see [Startup] for help. Pass your API key into maopts as a named element in a list like list(key = Sys.getenv('MICROSOFT_ACADEMIC_KEY')) Crossref: Crossref encourages requests with contact information (an email address) and will forward you to a dedicated API cluster for improved performance when you share your email address with them. https://github.com/CrossRef/rest-api-doc#good-manners--more-reliable-service To pass your email address to Crossref via this client, store it as an environment variable in .Renviron like crossref_email = name@example.com Entrez: NCBI limits users to making only 3 requests per second. But, users who register for an API key are able to make up to ten requests per second. Getting a key is simple; register for a “my ncbi” account then click on a button in the account settings page. Once you have an API key, you can pass it as the argument api_key to entrezopts in both [ft_get()] and [ft_search()]. However, we advise you use environment variables instead as they are more secure. To do that you can set an environment variable for the current R session like Sys.setenv(ENTREZ_KEY=&quot;yourkey&quot;) OR better yet set it in your .Renviron or equivalent file with an entry like ENTREZ_KEY=yourkey so that it is used across R sessions. Not needed for PLOS, eLife, arxiv, biorxiv, Euro PMC "],
["search.html", "Chapter 5 Search 5.1 Usage", " Chapter 5 Search Search is what you’ll likely start with for a number of reasons. First, search functionality in fulltext means that you can start from searching on words like ‘ecology’ or ‘cellular’ - and the output of that search can be fed downstream to the next major task: fetching articles. 5.1 Usage library(fulltext) List backends available ft_search_ls() #&gt; [1] &quot;arxiv&quot; &quot;biorxivr&quot; &quot;bmc&quot; &quot;crossref&quot; &quot;entrez&quot; #&gt; [6] &quot;europe_pmc&quot; &quot;ma&quot; &quot;plos&quot; &quot;scopus&quot; Search - by default searches against PLOS (Public Library of Science) res &lt;- ft_search(query = &quot;ecology&quot;) The output of ft_search is a ft S3 object, with a summary of the results: res #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 47337; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] and has slots for each data source: names(res) #&gt; [1] &quot;plos&quot; &quot;bmc&quot; &quot;crossref&quot; &quot;entrez&quot; &quot;arxiv&quot; &quot;biorxiv&quot; #&gt; [7] &quot;europmc&quot; &quot;scopus&quot; &quot;ma&quot; Get data for a single source res$plos #&gt; Query: [ecology] #&gt; Records found, returned: [47337, 10] #&gt; License: [CC-BY] #&gt; id #&gt; 1 10.1371/journal.pone.0001248 #&gt; 2 10.1371/journal.pone.0059813 #&gt; 3 10.1371/journal.pone.0155019 #&gt; 4 10.1371/journal.pone.0080763 #&gt; 5 10.1371/journal.pone.0208370 #&gt; 6 10.1371/journal.pone.0150648 #&gt; 7 10.1371/journal.pcbi.1003594 #&gt; 8 10.1371/journal.pone.0102437 #&gt; 9 10.1371/journal.pone.0175014 #&gt; 10 10.1371/journal.pone.0166559 "],
["abstract.html", "Chapter 6 Abstract 6.1 Usage", " Chapter 6 Abstract Abstracts likely will come after searching for articles with ft_search(). There are a few scenarios in which simply getting abstracts in lieu of full text may be enough. For example, if you know that a large portion of the articles you want to mine text from are closed access and you don’t have access to them, you may have access to the abstracts depending on the publisher. In addition, there are cases in which you really only need abstracts regardless of whether full text is available or not. ft_abstract() gives you access to the following data sources: crossref microsoft plos scopus 6.1 Usage library(fulltext) List data sources available ft_abstract_ls() #&gt; [1] &quot;crossref&quot; &quot;microsoft&quot; &quot;plos&quot; &quot;scopus&quot; Search - by default searches against PLOS (Public Library of Science) res &lt;- ft_search(query = &quot;ecology&quot;) (dois &lt;- res$plos$data$id) #&gt; [1] &quot;10.1371/journal.pone.0001248&quot; &quot;10.1371/journal.pone.0059813&quot; #&gt; [3] &quot;10.1371/journal.pone.0155019&quot; &quot;10.1371/journal.pone.0080763&quot; #&gt; [5] &quot;10.1371/journal.pone.0208370&quot; &quot;10.1371/journal.pone.0150648&quot; #&gt; [7] &quot;10.1371/journal.pcbi.1003594&quot; &quot;10.1371/journal.pone.0102437&quot; #&gt; [9] &quot;10.1371/journal.pone.0175014&quot; &quot;10.1371/journal.pone.0166559&quot; Take the output of ft_search() and pass to ft_abstract(): out &lt;- ft_abstract(dois) and has slots for each data source: names(out) #&gt; [1] &quot;plos&quot; &quot;scopus&quot; &quot;ma&quot; &quot;crossref&quot; "],
["links.html", "Chapter 7 Links 7.1 Usage", " Chapter 7 Links The ft_links function makes it easy to get URLs for full text versions of articles. You can for instance only use fulltext to pass DOIs directly to ft_links to get URLs to use elsewhere in your research workflow. Or you may want to search first with ft_search, then pass that output directly to ft_links. 7.1 Usage library(fulltext) List backends available ft_links_ls() #&gt; [1] &quot;bmc&quot; &quot;cogent&quot; &quot;copernicus&quot; &quot;crossref&quot; &quot;elife&quot; #&gt; [6] &quot;entrez&quot; &quot;frontiersin&quot; &quot;peerj&quot; &quot;plos&quot; You can pass DOIs directly to ft_links (res &lt;- ft_links(&#39;10.3389/fphar.2014.00109&#39;)) #&gt; &lt;fulltext links&gt; #&gt; [Found] 1 #&gt; [IDs] 10.3389/fphar.2014.00109 ... res$frontiersin #&gt; $found #&gt; [1] 1 #&gt; #&gt; $ids #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; #&gt; #&gt; $data #&gt; $data$`10.3389/fphar.2014.00109` #&gt; $data$`10.3389/fphar.2014.00109`$xml #&gt; [1] &quot;http://journal.frontiersin.org/article/10.3389/fphar.2014.00109/xml/nlm&quot; #&gt; #&gt; $data$`10.3389/fphar.2014.00109`$pdf #&gt; [1] &quot;http://journal.frontiersin.org/article/10.3389/fphar.2014.00109/pdf&quot; #&gt; #&gt; #&gt; #&gt; $opts #&gt; list() Or search first (res1 &lt;- ft_search(query=&#39;ecology&#39;, from=&#39;entrez&#39;)) #&gt; Query: #&gt; [ecology] #&gt; Found: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 168072; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] #&gt; Returned: #&gt; [PLoS: 0; BMC: 0; Crossref: 0; Entrez: 10; arxiv: 0; biorxiv: 0; Europe PMC: 0; Scopus: 0; Microsoft: 0] Then pass the output of that directly to ft_links (out &lt;- ft_links(res1)) #&gt; &lt;fulltext links&gt; #&gt; [Found] 8 #&gt; [IDs] ID_30949584 ID_30949543 ID_30926869 ID_30926867 ID_30926861 #&gt; ID_30926847 ID_30926838 ID_30926833 ID_30926825 ID_30926823 ... "],
["fetch.html", "Chapter 8 Fetch 8.1 Data formats 8.2 How data is stored 8.3 Usage 8.4 Errors 8.5 Cleanup 8.6 Internals 8.7 Notes about specific data sources", " Chapter 8 Fetch The ft_get function makes it easy to fetch full text articles. There are a few different ways to use ft_get: Pass in only DOIs - leave from parameter NULL. This route will first query Crossref API for the publisher of the DOI, then we’ll use the appropriate method to fetch full text from the publisher. If a publisher is not found for the DOI, then we’ll throw back a message telling you a publisher was not found. Pass in DOIs (or other pub IDs) and use the from parameter. This route means we don’t have to make an extra API call to Crossref (thus, this route is faster) to determine the publisher for each DOI. We go straight to getting full text based on the publisher. Use ft_search() to search for articles. Then pass that output to this function, which will use info in that object. This behaves the same as the previous option in that each DOI has publisher info so we know how to get full text for each DOI. Note that some publishers are available through other data sources, e.g., through Entrez’s Pubmed. ft_get is a bit complicated. These are just some of the hurdles we’re jumping over: Negotiating various user inputs, likely seeing new publishers we’ve not dealt with Dealing with authentication and trying to make it easier for users Users sometimes being at an IP address that has access to a publisher and sometimes not Caching results to avoid unnecessary downloads if the content has already been acquired Thus, expect some hiccups here, and please do report problems, and if a certain publisher is not supported yet. 8.1 Data formats You can specify whether you want PDF, XML or plaint text with the type parameter. It is sometimes ignored, sometimes used, depending on the data source. For certain data sources, they only accept one type. Details by data source/publisher: PLOS: pdf and xml Entrez: only xml eLife: pdf and xml Pensoft: pdf and xml arXiv: only pdf BiorXiv: only pdf Elsevier: pdf and plain Wiley: only pdf Peerj: pdf and xml Informa: only pdf FrontiersIn: pdf and xml Copernicus: pdf and xml Scientific Societies: only pdf Crossref: depends on the publisher other data sources/publishers: there are too many to cover here - will try to make a helper in the future for what is covered by different publishers 8.2 How data is stored This depends on what backend value you use. If you use the default (rds) we store all data in .rds files. These are binary compressed files that are specific to R. Because they are specific to R, you don’t want to use this option if part of your downstream workflow is using another tool/programming language. The three types are stored in differnt ways. xml and plain text are parsed to plain text then stored with whatever backend you choose. However, pdf is retrived as raw bytes and stored as such. Thus, we no longer write pdf files to disk. However, you can easily do that yourself with ft_extract() or yourself by using pdftools::pdf_text which accepts a file path to a pdf or raw bytes. 8.3 Usage library(fulltext) List backends available ft_get_ls() #&gt; [1] &quot;aaas&quot; &quot;aip&quot; &quot;amersocclinoncol&quot; #&gt; [4] &quot;amersocmicrobiol&quot; &quot;arxiv&quot; &quot;biorxiv&quot; #&gt; [7] &quot;bmc&quot; &quot;copernicus&quot; &quot;crossref&quot; #&gt; [10] &quot;elife&quot; &quot;elsevier&quot; &quot;entrez&quot; #&gt; [13] &quot;frontiersin&quot; &quot;ieee&quot; &quot;informa&quot; #&gt; [16] &quot;instinvestfil&quot; &quot;jama&quot; &quot;microbiology&quot; #&gt; [19] &quot;peerj&quot; &quot;pensoft&quot; &quot;plos&quot; #&gt; [22] &quot;pnas&quot; &quot;royalsocchem&quot; &quot;scientificsocieties&quot; #&gt; [25] &quot;wiley&quot; The simplest approach is passing a DOI directly to ft_get (res &lt;- ft_get(&#39;10.1371/journal.pone.0086169&#39;)) #&gt; &lt;fulltext text&gt; #&gt; [Docs] 1 #&gt; [Source] ext - /Users/sckott/Library/Caches/R/fulltext #&gt; [IDs] 10.1371/journal.pone.0086169 ... res$plos #&gt; $found #&gt; [1] 1 #&gt; #&gt; $dois #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $data #&gt; $data$backend #&gt; [1] &quot;ext&quot; #&gt; #&gt; $data$cache_path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext&quot; #&gt; #&gt; $data$path #&gt; $data$path$`10.1371/journal.pone.0086169` #&gt; $data$path$`10.1371/journal.pone.0086169`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_1371_journal_pone_0086169.xml&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$id #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.1371/journal.pone.0086169`$error #&gt; NULL #&gt; #&gt; #&gt; #&gt; $data$data #&gt; NULL #&gt; #&gt; #&gt; $opts #&gt; $opts$doi #&gt; [1] &quot;10.1371/journal.pone.0086169&quot; #&gt; #&gt; $opts$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $opts$progress #&gt; [1] FALSE #&gt; #&gt; #&gt; $errors #&gt; id error #&gt; 1 10.1371/journal.pone.0086169 &lt;NA&gt; You can pass many DOIs in at once (res &lt;- ft_get(c(&#39;10.3389/fphar.2014.00109&#39;, &#39;10.3389/feart.2015.00009&#39;))) #&gt; &lt;fulltext text&gt; #&gt; [Docs] 2 #&gt; [Source] ext - /Users/sckott/Library/Caches/R/fulltext #&gt; [IDs] 10.3389/fphar.2014.00109 10.3389/feart.2015.00009 ... res$frontiersin #&gt; $found #&gt; [1] 2 #&gt; #&gt; $dois #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $data #&gt; $data$backend #&gt; [1] &quot;ext&quot; #&gt; #&gt; $data$cache_path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext&quot; #&gt; #&gt; $data$path #&gt; $data$path$`10.3389/fphar.2014.00109` #&gt; $data$path$`10.3389/fphar.2014.00109`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_3389_fphar_2014_00109.xml&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$id #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.3389/fphar.2014.00109`$error #&gt; NULL #&gt; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009` #&gt; $data$path$`10.3389/feart.2015.00009`$path #&gt; [1] &quot;/Users/sckott/Library/Caches/R/fulltext/10_3389_feart_2015_00009.xml&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$id #&gt; [1] &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $data$path$`10.3389/feart.2015.00009`$error #&gt; NULL #&gt; #&gt; #&gt; #&gt; $data$data #&gt; NULL #&gt; #&gt; #&gt; $opts #&gt; $opts$dois #&gt; [1] &quot;10.3389/fphar.2014.00109&quot; &quot;10.3389/feart.2015.00009&quot; #&gt; #&gt; $opts$type #&gt; [1] &quot;xml&quot; #&gt; #&gt; $opts$progress #&gt; [1] FALSE #&gt; #&gt; #&gt; $errors #&gt; id error #&gt; 1 10.3389/fphar.2014.00109 &lt;NA&gt; #&gt; 2 10.3389/feart.2015.00009 &lt;NA&gt; 8.4 Errors ft_get() for each article has an error slot. If the error slot is NULL then there is no error. If the error slot is not NULL there was an error, and the error message will be a character string in that slot. Possible errors include: An error reported by the web service e.g. “Timeout was reached: Connection timed out after 10003 milliseconds” No link found e.g. “no link found from Crossref” OR “has no link available” We attempted to fetch the article but the content type wasn’t what was expected. In this case we skip to the next article. e.g. “type was supposed to be pdf, but was text/html; charset=UTF-8” Weird uninformative errors e.g. “Recv failure: Operation timed out” OR “Operation was aborted by an application callback” An error associated mostly with PLOS. PLOS gives DOIs for parts of articles, like figures, so it doesn’t make sense to get full text of a figure. e.g., “was not found or may be a DOI for a part of an article” An additional top level slot called errors has a data.frame of all errors from each article, like: res &lt;- ft_get(c(&#39;10.7554/eLife.03032&#39;, &#39;10.7554/eLife.aaaa&#39;), from = &quot;elife&quot;) res$elife$errors id error 1 10.7554/eLife.03032 &lt;NA&gt; 2 10.7554/eLife.aaaa subscript out of bounds Where the second DOI was invalid. Granted the error in the data.frame “subscript out of bounds” isn’t very informative, but we can work on that. 8.5 Cleanup The above section about errors suggests that we often run into errors. When we run into errors downloading full text we capture the error message, if there is one, and delete the file we were trying to create. That is, we cleanup upon hitting an error such that you shouldn’t end up with blank files on your machine. Let us know if this isn’t true in your case and we’ll get it fixed. Note that even if you exit out of the all to ft_get() it should clean up a file if it is not completey done creating it, so you shouldn’t end up with bad files if you exit out of the function when it’s running. 8.6 Internals What’s going on under the hood in ft_get()? It goes like this: If you request data from a specific data source (use of from, only allowed for PLOS, Entrez, eLife, Pensoft, arXiv, BiorXiv, Elsevier and Wiley): Grab publisher specific collector function Each of these functions has specific code for that publisher to pull full text given an article identifier If you don’t request a specific data source: Guess which publisher the DOI comes from If publisher discovered that we have plugins for Get publisher specific collector function If publisher not discovered Ping the https://ftdoi.org API If publisher found with ftdoi API: Link for full text used from the ftdoi API response If publisher not found with ftdoi API: Attempt fetch via Crossref API - if links found we try those, some work and some don’t 8.7 Notes about specific data sources 8.7.1 Elsevier When you don’t have access to the full text of Elsevier articles they will often still give you something, but it will sometimes be just metadata of the paper or sometimes an abstract if you’re lucky. When you go to extract the text this will be rather obvious. "],
["chunks.html", "Chapter 9 Chunks 9.1 Usage 9.2 Tabularize", " Chapter 9 Chunks The ft_chunks function tries to make it easy to extract the parts of articles you want. This only works with XML format articles though since although we can get text out of PDFs, there is no machine readable way to say “I want the abstract”. In addition to only working with XML, this function only has knowledge about a few select publishers for which we’ve coded knowledge about how to get different sections of the article. Not all publishers use the same format XML - so each publisher is slightly different for how to get to each section. That is, to get to the abstract requires slightly different xpath for publisher A vs. publisher B vs. publisher C. 9.1 Usage library(fulltext) library(pubchunks) Get a full text article x &lt;- ft_get(&#39;10.1371/journal.pone.0086169&#39;, from=&#39;plos&#39;) Note that unlike previous version of fulltext you now have to collect (ft_collect()) the text from the XML file on disk. Then you can pass to pub_chunks(), here to get authors. x %&gt;% ft_collect %&gt;% pub_chunks(&quot;authors&quot;) #&gt; $plos #&gt; $plos$`10.1371/journal.pone.0086169` #&gt; &lt;pub chunks&gt; #&gt; from: xml_document #&gt; publisher/journal: plos/PLoS ONE #&gt; sections: authors #&gt; showing up to first 5: #&gt; authors (n=4): nested list #&gt; #&gt; #&gt; attr(,&quot;ft_data&quot;) #&gt; [1] TRUE In another example, let’s search for PLOS articles. library(&quot;rplos&quot;) (dois &lt;- searchplos(q=&quot;*:*&quot;, fl=&#39;id&#39;, fq=list(&#39;doc_type:full&#39;,&quot;article_type:\\&quot;research article\\&quot;&quot;), limit=5)$data$id) #&gt; [1] &quot;10.1371/journal.pone.0155491&quot; &quot;10.1371/journal.pone.0168627&quot; #&gt; [3] &quot;10.1371/journal.pone.0184491&quot; &quot;10.1371/journal.pone.0155489&quot; #&gt; [5] &quot;10.1371/journal.pone.0127059&quot; Then get the full text x &lt;- ft_get(dois, from=&quot;plos&quot;) Then pull out various sections of each article. remember to pull out the full text first x &lt;- ft_collect(x) x %&gt;% pub_chunks(&quot;front&quot;) x %&gt;% pub_chunks(&quot;body&quot;) x %&gt;% pub_chunks(&quot;back&quot;) x %&gt;% pub_chunks(&quot;history&quot;) x %&gt;% pub_chunks(&quot;authors&quot;) x %&gt;% pub_chunks(c(&quot;doi&quot;,&quot;categories&quot;)) x %&gt;% pub_chunks(&quot;all&quot;) x %&gt;% pub_chunks(&quot;publisher&quot;) x %&gt;% pub_chunks(&quot;acknowledgments&quot;) x %&gt;% pub_chunks(&quot;permissions&quot;) x %&gt;% pub_chunks(&quot;journal_meta&quot;) x %&gt;% pub_chunks(&quot;article_meta&quot;) 9.2 Tabularize The function pub_tabularize() is useful for coercing the output of pub_chunks() into a data.frame, the lingua franca of data work in R. library(data.table) x &lt;- pub_chunks(x, c(&quot;doi&quot;, &quot;title&quot;)) x &lt;- pub_tabularize(x) rbindlist(x$plos, fill = TRUE) #&gt; doi #&gt; 1: 10.1371/journal.pone.0155491 #&gt; 2: 10.1371/journal.pone.0168627 #&gt; 3: 10.1371/journal.pone.0184491 #&gt; 4: 10.1371/journal.pone.0155489 #&gt; 5: 10.1371/journal.pone.0127059 #&gt; title #&gt; 1: Uterine Expression of NDRG4 Is Induced by Estrogen and Up-Regulated during Embryo Implantation Process in Mice #&gt; 2: Galectin-3 and Its Genetic Variation rs4644 Modulate Enterovirus 71 Infection #&gt; 3: Ethics approval in applications for open-access clinical trial data: An analysis of researcher statements to clinicalstudydatarequest.com #&gt; 4: Risk Factors for Distant Metastasis in Patients with Minimally Invasive Follicular Thyroid Carcinoma #&gt; 5: Seasonal Differences in Extinction and Colonization Drive Occupancy Dynamics of an Imperilled Amphibian #&gt; .publisher #&gt; 1: plos #&gt; 2: plos #&gt; 3: plos #&gt; 4: plos #&gt; 5: plos "],
["table.html", "Chapter 10 Table 10.1 Usage", " Chapter 10 Table The ft_table() function makes it easy to create a data.frame of the text of PDF, plain text, and XML files, together with DOIs/IDs for each article. It’s similar to the readtext::readtext() function, but is much more specific to just this package. With the output of ft_table() you can go directly into a text-mining package like quanteda. 10.1 Usage library(fulltext) Use ft_table() to pull out text from all articles. ft_table() #&gt; # A tibble: 19 x 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1371/jo… 10_1371_journ… plosPLoS Comput Biolp… /Users/sckott/Library… #&gt; 2 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 3 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 4 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 5 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 6 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 7 10.1371/jo… 10_1371_journ… &quot;PLoS ONEplosplosoneP… /Users/sckott/Library… #&gt; 8 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 9 10.1371/jo… 10_1371_journ… &quot;PLoS ONEplosplosoneP… /Users/sckott/Library… #&gt; 10 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 11 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 12 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 13 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 14 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 15 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 16 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 17 10.3389/fe… 10_3389_feart… Front. Earth Sci.Fron… /Users/sckott/Library… #&gt; 18 10.3389/fp… 10_3389_fphar… &quot;Front. Pharmacol.Fro… /Users/sckott/Library… #&gt; 19 cond-mat/9… cond_mat_9309… &quot; … /Users/sckott/Library… You can pull out just text from XML files ft_table(type = &quot;xml&quot;) #&gt; # A tibble: 18 x 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1371/jo… 10_1371_journ… plosPLoS Comput Biolp… /Users/sckott/Library… #&gt; 2 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 3 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 4 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 5 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 6 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 7 10.1371/jo… 10_1371_journ… &quot;PLoS ONEplosplosoneP… /Users/sckott/Library… #&gt; 8 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 9 10.1371/jo… 10_1371_journ… &quot;PLoS ONEplosplosoneP… /Users/sckott/Library… #&gt; 10 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 11 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 12 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 13 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 14 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 15 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 16 10.1371/jo… 10_1371_journ… PLoS ONEplosplosonePL… /Users/sckott/Library… #&gt; 17 10.3389/fe… 10_3389_feart… Front. Earth Sci.Fron… /Users/sckott/Library… #&gt; 18 10.3389/fp… 10_3389_fphar… &quot;Front. Pharmacol.Fro… /Users/sckott/Library… You can pull out just text from PDF files ft_table(type = &quot;pdf&quot;) #&gt; # A tibble: 1 x 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 cond-mat… cond_mat_9… &quot; … /Users/sckott/Library/… You can pull out XML but not extract the text. So you’ll get XML strings that you can parse yourself with xpath/css selectors/etc. ft_table(xml_extract_text = FALSE) #&gt; # A tibble: 19 x 4 #&gt; dois ids_norm text paths #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 2 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 3 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 4 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 5 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 6 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 7 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 8 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 9 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 10 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 11 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 12 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 13 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 14 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 15 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 16 10.1371/jo… 10_1371_journ… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 17 10.3389/fe… 10_3389_feart… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 18 10.3389/fp… 10_3389_fphar… &quot;&lt;?xml version=\\&quot;1.0\\… /Users/sckott/Library… #&gt; 19 cond-mat/9… cond_mat_9309… &quot; … /Users/sckott/Library… "],
["supplementary.html", "Chapter 11 Supplementary", " Chapter 11 Supplementary coming soon … "],
["use-cases.html", "Chapter 12 Use cases", " Chapter 12 Use cases Coming soon … "],
["literature.html", "Chapter 13 Literature", " Chapter 13 Literature Here is a review of existing methods. "]
]
